{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from math import gamma\n",
    "\n",
    "\n",
    "class LSTM_Encoding_Action(nn.Module):\n",
    "\n",
    "    '''\n",
    "    Parameters:\n",
    "        input_size:\n",
    "        hidden_size:\n",
    "        output_size:\n",
    "        num_layers:\n",
    "    '''\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers:int = 1) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers)    # Utilize the LSTM model in torch.nn\n",
    "        self.linear1 = nn.Linear(hidden_size, output_size)          # fully connected layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, _  = self.lstm(x)\n",
    "        seq_length, batch_size, hidden_size = x.shape\n",
    "        x = x.view(seq_length*batch_size, hidden_size)\n",
    "        x = self.linear1(x)\n",
    "        x = x.view(seq_length, batch_size, -1)\n",
    "        return x\n",
    "\n",
    "\n",
    "class LSTM_Encoding_History(nn.Module):\n",
    "\n",
    "    '''\n",
    "    NOTE:Returns a categorical distribution\n",
    "\n",
    "    Parameters:\n",
    "        input_size:\n",
    "        hidden_size:\n",
    "        output_size:\n",
    "        num_layers\n",
    "    '''\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers: int = 1) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers)\n",
    "        self.linear1 = nn.Linear(hidden_size, output_size, bias=True)\n",
    "        self.softmax = nn.Softmax(dim = output_size)\n",
    "\n",
    "    def forward(self, x, action_embedding: torch.tensor):\n",
    "        '''\n",
    "        Parameters:\n",
    "            x: mental history\n",
    "            action_embedding: encoding of action history. This should be the output of LSTM_Encoding_Action\n",
    "        '''\n",
    "        x, _ = self.lstm(x)\n",
    "        x = torch.concat(tensors=[x, action_embedding])     #NOTE: concatenate the action info and the mental info\n",
    "        seq_length, batch_size, hidden_size = x.shape\n",
    "        x = x.view(seq_length*batch_size, hidden_size)\n",
    "        x = self.linear1(x)\n",
    "        x = x.view(seq_length, batch_size, -1)\n",
    "        #TODO: return a vector with dimension (I+1), (I represents the number of types of mental states)\n",
    "        #TODO: the i-th (i=0,1,2,...,I) component of the output x represents the probability of the i-th mental type\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "        \n",
    "\n",
    "class Logic_Model_Incomplete_Data:\n",
    "\n",
    "    def __init__(self, time_horizon:float, partition_size:float) -> None:\n",
    "        self.time_horizon = time_horizon\n",
    "        self.partition_size = partition_size            # num of small time intervals\n",
    "\n",
    "        #TODO:\n",
    "        ### the following parameters are used to manually define the logic rules\n",
    "        self.num_predicate = 7                  # num_predicate is same as num_node\n",
    "        self.num_formula = 8                    # num of prespecified logic rules\n",
    "        self.BEFORE = 'BEFORE'\n",
    "        self.EQUAL = 'EQUAL'\n",
    "        self.AFTER = 'AFTER'\n",
    "        self.Time_tolerance = 0.3               \n",
    "        self.body_predicate_set = []                        # the index set of all body predicates\n",
    "        self.mental_predicate_set = [0, 1, 2]\n",
    "        self.action_predicate_set = [3, 4, 5, 6]\n",
    "        self.head_predicate_set = [0, 1, 2, 3, 4, 5, 6]     # the index set of all head predicates\n",
    "        self.decay_rate = 1                                 # decay kernel\n",
    "        self.integral_resolution = 0.03\n",
    "\n",
    "        ### the following parameters are used to generate synthetic data\n",
    "        ### for the learning part, the following is used to claim variables\n",
    "        ### self.model_parameter = {0:{},1:{},...,6:{}}\n",
    "        self.model_parameter = {}\n",
    "\n",
    "\n",
    "        '''\n",
    "        mental\n",
    "        '''\n",
    "\n",
    "        head_predicate_idx = 0\n",
    "        self.model_parameter[head_predicate_idx] = {}\n",
    "        self.model_parameter[head_predicate_idx]['base'] = torch.autograd.Variable((torch.ones(1) * -0.3).double(), requires_grad=True)\n",
    "\n",
    "        formula_idx = 0\n",
    "        self.model_parameter[head_predicate_idx][formula_idx] = {}\n",
    "        self.model_parameter[head_predicate_idx][formula_idx]['weight'] = torch.autograd.Variable((torch.ones(1) * 0.01).double(), requires_grad=True)\n",
    "        formula_idx = 1\n",
    "        self.model_parameter[head_predicate_idx][formula_idx] = {}\n",
    "        self.model_parameter[head_predicate_idx][formula_idx]['weight'] = torch.autograd.Variable((torch.ones(1) * 0.01).double(), requires_grad=True)\n",
    "\n",
    "        head_predicate_idx = 1\n",
    "        self.model_parameter[head_predicate_idx] = {}\n",
    "        self.model_parameter[head_predicate_idx]['base'] = torch.autograd.Variable((torch.ones(1) * -0.3).double(), requires_grad=True)\n",
    "\n",
    "        formula_idx = 0\n",
    "        self.model_parameter[head_predicate_idx][formula_idx] = {}\n",
    "        self.model_parameter[head_predicate_idx][formula_idx]['weight'] = torch.autograd.Variable((torch.ones(1) * 0.02).double(), requires_grad=True)\n",
    "        formula_idx = 1\n",
    "        self.model_parameter[head_predicate_idx][formula_idx] = {}\n",
    "        self.model_parameter[head_predicate_idx][formula_idx]['weight'] = torch.autograd.Variable((torch.ones(1) * 0.01).double(), requires_grad=True)\n",
    "\n",
    "        head_predicate_idx = 2\n",
    "        self.model_parameter[head_predicate_idx] = {}\n",
    "        self.model_parameter[head_predicate_idx]['base'] = torch.autograd.Variable((torch.ones(1) * -0.2).double(), requires_grad=True)\n",
    "\n",
    "        formula_idx = 0\n",
    "        self.model_parameter[head_predicate_idx][formula_idx] = {}\n",
    "        self.model_parameter[head_predicate_idx][formula_idx]['weight'] = torch.autograd.Variable((torch.ones(1) * 0.1).double(), requires_grad=True)\n",
    "\n",
    "\n",
    "        '''\n",
    "        action\n",
    "        '''\n",
    "        head_predicate_idx = 3\n",
    "        self.model_parameter[head_predicate_idx] = {}\n",
    "        self.model_parameter[head_predicate_idx]['base'] = torch.autograd.Variable((torch.ones(1) * -0.1).double(), requires_grad=True)\n",
    "\n",
    "        formula_idx = 0\n",
    "        self.model_parameter[head_predicate_idx][formula_idx] = {}\n",
    "        self.model_parameter[head_predicate_idx][formula_idx]['weight'] = torch.autograd.Variable((torch.ones(1) * 0.01).double(), requires_grad=True)\n",
    "\n",
    "        head_predicate_idx = 4\n",
    "        self.model_parameter[head_predicate_idx] = {}\n",
    "        self.model_parameter[head_predicate_idx]['base'] = torch.autograd.Variable((torch.ones(1) * -0.25).double(), requires_grad=True)\n",
    "\n",
    "        formula_idx = 0\n",
    "        self.model_parameter[head_predicate_idx][formula_idx] = {}\n",
    "        self.model_parameter[head_predicate_idx][formula_idx]['weight'] = torch.autograd.Variable((torch.ones(1) * 0.05).double(), requires_grad=True)\n",
    "\n",
    "        head_predicate_idx = 5\n",
    "        self.model_parameter[head_predicate_idx] = {}\n",
    "        self.model_parameter[head_predicate_idx]['base'] = torch.autograd.Variable((torch.ones(1) * -0.6).double(), requires_grad=True)\n",
    "\n",
    "        formula_idx = 0\n",
    "        self.model_parameter[head_predicate_idx][formula_idx] = {}\n",
    "        self.model_parameter[head_predicate_idx][formula_idx]['weight'] = torch.autograd.Variable((torch.ones(1) * 0.8).double(), requires_grad=True)\n",
    "\n",
    "        head_predicate_idx = 6\n",
    "        self.model_parameter[head_predicate_idx] = {}\n",
    "        self.model_parameter[head_predicate_idx]['base'] = torch.autograd.Variable((torch.ones(1) * -0.1).double(), requires_grad=True)\n",
    "\n",
    "        formula_idx = 0\n",
    "        self.model_parameter[head_predicate_idx][formula_idx] = {}\n",
    "        self.model_parameter[head_predicate_idx][formula_idx]['weight'] = torch.autograd.Variable((torch.ones(1) * 0.05).double(), requires_grad=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        #NOTE: set the content of logic rules\n",
    "        self.logic_template = self.logic_rule()\n",
    "    \n",
    "    def logic_rule(self):\n",
    "        #TODO: the logic rules encode the prior knowledge\n",
    "        # encode rule information\n",
    "        '''\n",
    "        This function encodes the content of logic rules\n",
    "        logic_template = {0:{},1:{},...,6:{}}\n",
    "        '''\n",
    "        logic_template = {}\n",
    "\n",
    "\n",
    "        '''\n",
    "        Mental (0-2)\n",
    "        '''\n",
    "\n",
    "        head_predicate_idx = 0\n",
    "        logic_template[head_predicate_idx] = {} # here 0 is the index of the head predicate; we could have multiple head predicates\n",
    "\n",
    "        #NOTE: rule content: (2 and 3 and 4) and before(2,0) and before(3,0) and before(4,0) \\to 0\n",
    "        formula_idx = 0\n",
    "        logic_template[head_predicate_idx][formula_idx] = {}\n",
    "        logic_template[head_predicate_idx][formula_idx]['body_predicate_idx'] = [2,3,4]\n",
    "        logic_template[head_predicate_idx][formula_idx]['body_predicate_sign'] = [1, 1, 1]  # use 1 to indicate True; use -1 to indicate False\n",
    "        logic_template[head_predicate_idx][formula_idx]['head_predicate_sign'] = [1]\n",
    "        logic_template[head_predicate_idx][formula_idx]['temporal_relation_idx'] = [[2, 0], [3, 0], [4, 0]]\n",
    "        logic_template[head_predicate_idx][formula_idx]['temporal_relation_type'] = [self.BEFORE, self.BEFORE, self.BEFORE]\n",
    "\n",
    "\n",
    "        #NOTE: rule content: ((\\neg 0 and (2 and 6)) and after(6,0) and equal(2,0) \\to \\neg 0)\n",
    "        formula_idx = 1\n",
    "        logic_template[head_predicate_idx][formula_idx] = {}\n",
    "        logic_template[head_predicate_idx][formula_idx]['body_predicate_idx'] = [0, 2, 6]\n",
    "        logic_template[head_predicate_idx][formula_idx]['body_predicate_sign'] = [-1, 1, 1]  # use 1 to indicate True; use -1 to indicate False\n",
    "        logic_template[head_predicate_idx][formula_idx]['head_predicate_sign'] = [-1]\n",
    "        logic_template[head_predicate_idx][formula_idx]['temporal_relation_idx'] = [[6, 0], [2, 0]]\n",
    "        logic_template[head_predicate_idx][formula_idx]['temporal_relation_type'] = [self.AFTER, self.EQUAL]\n",
    "\n",
    "        head_predicate_idx = 1\n",
    "        logic_template[head_predicate_idx] = {}  # here 1 is the index of the head predicate; we could have multiple head predicates\n",
    "\n",
    "        #NOTE: rule content: 5 and before(5,1) to 1\n",
    "        formula_idx = 0\n",
    "        logic_template[head_predicate_idx][formula_idx] = {}\n",
    "        logic_template[head_predicate_idx][formula_idx]['body_predicate_idx'] = [5]\n",
    "        logic_template[head_predicate_idx][formula_idx]['body_predicate_sign'] = [1]\n",
    "        logic_template[head_predicate_idx][formula_idx]['head_predicate_sign'] = [1]\n",
    "        logic_template[head_predicate_idx][formula_idx]['temporal_relation_idx'] = [[5, 1]]\n",
    "        logic_template[head_predicate_idx][formula_idx]['temporal_relation_type'] = [self.BEFORE]\n",
    "\n",
    "        #NOTE: rule content: (4 and 6) and before(6,1) to \\neg 1\n",
    "        formula_idx = 1\n",
    "        logic_template[head_predicate_idx][formula_idx] = {}\n",
    "        logic_template[head_predicate_idx][formula_idx]['body_predicate_idx'] = [4, 6]\n",
    "        logic_template[head_predicate_idx][formula_idx]['body_predicate_sign'] = [1, 1]\n",
    "        logic_template[head_predicate_idx][formula_idx]['head_predicate_sign'] = [-1]\n",
    "        logic_template[head_predicate_idx][formula_idx]['temporal_relation_idx'] = [[6, 1]]\n",
    "        logic_template[head_predicate_idx][formula_idx]['temporal_relation_type'] = [self.BEFORE]\n",
    "\n",
    "\n",
    "        head_predicate_idx = 2\n",
    "        logic_template[head_predicate_idx] = {}  # here 2 is the index of the head predicate; we could have multiple head predicates\n",
    "\n",
    "        #NOTE: rule content: (\\neg 1 and 6) and after(1,2) to 2\n",
    "        formula_idx = 0\n",
    "        logic_template[head_predicate_idx][formula_idx] = {}\n",
    "        logic_template[head_predicate_idx][formula_idx]['body_predicate_idx'] = [1, 6]\n",
    "        logic_template[head_predicate_idx][formula_idx]['body_predicate_sign'] = [-1, 1]\n",
    "        logic_template[head_predicate_idx][formula_idx]['head_predicate_sign'] = [1]\n",
    "        logic_template[head_predicate_idx][formula_idx]['temporal_relation_idx'] = [[1, 2]]\n",
    "        logic_template[head_predicate_idx][formula_idx]['temporal_relation_type'] = [self.AFTER]\n",
    "\n",
    "\n",
    "\n",
    "        '''\n",
    "        Action (3-6)\n",
    "        '''\n",
    "        head_predicate_idx = 3\n",
    "        logic_template[head_predicate_idx] = {}  # here 3 is the index of the head predicate; we could have multiple head predicates\n",
    "\n",
    "        #NOTE: rule content: (0 and \\neg 1) and before(0,1) and before(1,3) \\to 3\n",
    "        formula_idx = 0\n",
    "        logic_template[head_predicate_idx][formula_idx] = {}\n",
    "        logic_template[head_predicate_idx][formula_idx]['body_predicate_idx'] = [0, 1]\n",
    "        logic_template[head_predicate_idx][formula_idx]['body_predicate_sign'] = [1, -1]\n",
    "        logic_template[head_predicate_idx][formula_idx]['head_predicate_sign'] = [1]\n",
    "        logic_template[head_predicate_idx][formula_idx]['temporal_relation_idx'] = [[0, 1], [1, 3]]\n",
    "        logic_template[head_predicate_idx][formula_idx]['temporal_relation_type'] = [self.BEFORE, self.BEFORE]\n",
    "\n",
    "\n",
    "        head_predicate_idx = 4\n",
    "        logic_template[head_predicate_idx] = {}  # here 4 is the index of the head predicate; we could have multiple head predicates\n",
    "\n",
    "        #NOTE: rule content: (2) and before(2,4) \\to 4\n",
    "        formula_idx = 0\n",
    "        logic_template[head_predicate_idx][formula_idx] = {}\n",
    "        logic_template[head_predicate_idx][formula_idx]['body_predicate_idx'] = [2]\n",
    "        logic_template[head_predicate_idx][formula_idx]['body_predicate_sign'] = [1]\n",
    "        logic_template[head_predicate_idx][formula_idx]['head_predicate_sign'] = [1]\n",
    "        logic_template[head_predicate_idx][formula_idx]['temporal_relation_idx'] = [[2, 4]]\n",
    "        logic_template[head_predicate_idx][formula_idx]['temporal_relation_type'] = [self.BEFORE]\n",
    "\n",
    "        \n",
    "        head_predicate_idx = 5\n",
    "        logic_template[head_predicate_idx] = {}  # here 5 is the index of the head predicate; we could have multiple head predicates\n",
    "\n",
    "        #NOTE: rule content: (0 and \\neg 1) and before(0,5) and after(1,5) \\to 5\n",
    "        formula_idx = 0\n",
    "        logic_template[head_predicate_idx][formula_idx] = {}\n",
    "        logic_template[head_predicate_idx][formula_idx]['body_predicate_idx'] = [0, 1]\n",
    "        logic_template[head_predicate_idx][formula_idx]['body_predicate_sign'] = [1, -1]\n",
    "        logic_template[head_predicate_idx][formula_idx]['head_predicate_sign'] = [1]\n",
    "        logic_template[head_predicate_idx][formula_idx]['temporal_relation_idx'] = [[0, 5], [1, 5]]\n",
    "        logic_template[head_predicate_idx][formula_idx]['temporal_relation_type'] = [self.BEFORE, self.AFTER]\n",
    "\n",
    "\n",
    "        head_predicate_idx = 6\n",
    "        logic_template[head_predicate_idx] = {}  # here 6 is the index of the head predicate; we could have multiple head predicates\n",
    "\n",
    "        #NOTE: rule content: (1 and 2) and before(1,6) and before(2,6) \\to \\neg 6\n",
    "        formula_idx = 0\n",
    "        logic_template[head_predicate_idx][formula_idx] = {}\n",
    "        logic_template[head_predicate_idx][formula_idx]['body_predicate_idx'] = [1, 2]\n",
    "        logic_template[head_predicate_idx][formula_idx]['body_predicate_sign'] = [1, 1]\n",
    "        logic_template[head_predicate_idx][formula_idx]['head_predicate_sign'] = [-1]\n",
    "        logic_template[head_predicate_idx][formula_idx]['temporal_relation_idx'] = [[1, 6], [2, 6]]\n",
    "        logic_template[head_predicate_idx][formula_idx]['temporal_relation_type'] = [self.BEFORE, self.BEFORE]\n",
    "\n",
    "\n",
    "        return logic_template\n",
    "\n",
    "    def intensity(self, cur_time, head_predicate_idx, history)->torch.tensor:\n",
    "        feature_formula = []\n",
    "        weight_formula = []\n",
    "        effect_formula = []\n",
    "        #TODO: Check if the head_prediate is a mental predicate\n",
    "        if head_predicate_idx in self.mental_predicate_set: flag = 0\n",
    "        else: flag = 1  #NOTE: action\n",
    "\n",
    "        for formula_idx in list(self.logic_template[head_predicate_idx].keys()):\n",
    "            weight_formula.append(self.model_parameter[head_predicate_idx][formula_idx]['weight'])\n",
    "\n",
    "            feature_formula.append(self.get_feature(cur_time=cur_time, head_predicate_idx=head_predicate_idx,\n",
    "                                                    history=history, template=self.logic_template[head_predicate_idx][formula_idx], flag=flag))\n",
    "            effect_formula.append(self.get_formula_effect(cur_time=cur_time, head_predicate_idx=head_predicate_idx,\n",
    "                                                       history=history, template=self.logic_template[head_predicate_idx][formula_idx]))\n",
    "        intensity = torch.exp(torch.cat(weight_formula, dim=0))/torch.sum(torch.exp(torch.cat(weight_formula, dim=0)), dim=0) * torch.cat(feature_formula, dim=0) * torch.cat(effect_formula, dim=0)\n",
    "        intensity = self.model_parameter[head_predicate_idx]['base'] + torch.sum(intensity)\n",
    "        intensity = torch.exp(intensity)\n",
    "\n",
    "        return intensity\n",
    "\n",
    "    def get_feature(self, cur_time, head_predicate_idx, history, template, flag:int):\n",
    "        #NOTE: flag: 0 or 1, denotes the head_predicate_idx is a mental or an action\n",
    "        #NOTE: 0 for mental and 1 for action\n",
    "        #NOTE: since for mental, we need to go through all the history information\n",
    "        #NOTE: while for action, we only care about the current time information\n",
    "        \n",
    "        transition_time_dic = {}\n",
    "        feature = torch.tensor([0], dtype=torch.float64)\n",
    "        for idx, body_predicate_idx in enumerate(template['body_predicate_idx']):\n",
    "            transition_time = np.array(history[body_predicate_idx]['time'])\n",
    "            transition_state = np.array(history[body_predicate_idx]['state'])\n",
    "            mask = (transition_time <= cur_time) * (transition_state == template['body_predicate_sign'][idx])\n",
    "            transition_time_dic[body_predicate_idx] = transition_time[mask]\n",
    "        transition_time_dic[head_predicate_idx] = [cur_time]\n",
    "        ### get weights\n",
    "        # compute features whenever any item of the transition_item_dic is nonempty\n",
    "        history_transition_len = [len(i) for i in transition_time_dic.values()]\n",
    "        if min(history_transition_len) > 0:\n",
    "            # need to compute feature using logic rules\n",
    "            time_combination = np.array(list(itertools.product(*transition_time_dic.values())))\n",
    "            time_combination_dic = {}\n",
    "            for i, idx in enumerate(list(transition_time_dic.keys())):\n",
    "                #TODO: this is where we distinguish mental and action\n",
    "                time_combination_dic[idx] = time_combination[:, i] if flag == 0 else time_combination[-1, i]\n",
    "            temporal_kernel = np.ones(len(time_combination))\n",
    "            for idx, temporal_relation_idx in enumerate(template['temporal_relation_idx']):       \n",
    "                #TODO: checkpoint\n",
    "                #print('head_predicate_idx: {}; temporal_relation_idx[0]: {}, temporal_relation_idx[1]: {}'.format(head_predicate_idx, temporal_relation_idx[0], temporal_relation_idx[1]))\n",
    "                #print('temporal combination dict: {}'.format(time_combination_dic))\n",
    "         \n",
    "                time_difference = time_combination_dic[temporal_relation_idx[0]] - time_combination_dic[temporal_relation_idx[1]]\n",
    "                if template['temporal_relation_type'][idx] == 'BEFORE':\n",
    "                    temporal_kernel *= (time_difference < - self.Time_tolerance) * np.exp(-self.decay_rate *(cur_time - time_combination_dic[temporal_relation_idx[0]]))\n",
    "                if template['temporal_relation_type'][idx] == 'EQUAL':\n",
    "                    temporal_kernel *= (abs(time_difference) <= self.Time_tolerance) * np.exp(-self.decay_rate*(cur_time - time_combination_dic[temporal_relation_idx[0]]))\n",
    "                if template['temporal_relation_type'][idx] == 'AFTER':\n",
    "                    temporal_kernel *= (time_difference > self.Time_tolerance) * np.exp(-self.decay_rate*(cur_time - time_combination_dic[temporal_relation_idx[1]]))\n",
    "            feature = torch.tensor([np.sum(temporal_kernel)], dtype=torch.float64)\n",
    "        return feature\n",
    "\n",
    "    def get_formula_effect(self, cur_time, head_predicate_idx, history, template):\n",
    "        ## Note this part is very important!! For generator, this should be np.sum(cur_time > head_transition_time) - 1\n",
    "        ## Since at the transition times, choose the intensity function right before the transition time\n",
    "        head_transition_time = np.array(history[head_predicate_idx]['time'])\n",
    "        head_transition_state = np.array(history[head_predicate_idx]['state'])\n",
    "        if len(head_transition_time) == 0:\n",
    "            cur_state = 0\n",
    "            counter_state = 1 - cur_state\n",
    "        else:\n",
    "            idx = np.sum(cur_time > head_transition_time) - 1\n",
    "            cur_state = head_transition_state[idx]\n",
    "            counter_state = 1 - cur_state\n",
    "        if counter_state == template['head_predicate_sign']:\n",
    "            formula_effect = torch.tensor([1], dtype=torch.float64)\n",
    "        else:\n",
    "            formula_effect = torch.tensor([-1], dtype=torch.float64)\n",
    "        return formula_effect\n",
    "\n",
    "    def log_likelihood(self, dataset, sample_ID_batch, T_max)->torch.tensor:\n",
    "        '''\n",
    "        This function calculates the log-likehood given the dataset\n",
    "        log-likelihood = \\sum log(intensity(transition_time)) + int_0^T intensity dt\n",
    "\n",
    "        Parameters:\n",
    "            dataset: \n",
    "            sample_ID_batch: list\n",
    "            T_max:\n",
    "        '''\n",
    "        log_likelihood = torch.tensor([0], dtype=torch.float64)\n",
    "        # iterate over samples\n",
    "        for sample_ID in sample_ID_batch:\n",
    "            # iterate over head predicates; each predicate corresponds to one intensity\n",
    "            data_sample = dataset[sample_ID]\n",
    "            for head_predicate_idx in self.head_predicate_set:\n",
    "                #NOTE: compute the summation of log intensities at the transition times\n",
    "                intensity_log_sum = self.intensity_log_sum(head_predicate_idx, data_sample)\n",
    "                #NOTE: compute the integration of intensity function over the time horizon\n",
    "                intensity_integral = self.intensity_integral(head_predicate_idx, data_sample, T_max)\n",
    "                log_likelihood += (intensity_log_sum - intensity_integral)\n",
    "        return log_likelihood\n",
    "\n",
    "    def intensity_log_sum(self, head_predicate_idx, data_sample):\n",
    "        intensity_transition = []\n",
    "        for t in data_sample[head_predicate_idx]['time'][1:]:\n",
    "            #NOTE: compute the intensity at transition times\n",
    "            cur_intensity:torch.tensor = self.intensity(t, head_predicate_idx, data_sample)\n",
    "            intensity_transition.append(cur_intensity)\n",
    "        if len(intensity_transition) == 0: # only survival term, no event happens\n",
    "            log_sum = torch.tensor([0], dtype=torch.float64)\n",
    "        else:\n",
    "            log_sum = torch.sum(torch.log(torch.cat(intensity_transition, dim=0)))\n",
    "        return log_sum\n",
    "\n",
    "    def intensity_integral(self, head_predicate_idx, data_sample, T_max):\n",
    "        start_time = 0\n",
    "        end_time = T_max\n",
    "        intensity_grid = []\n",
    "        for t in np.arange(start_time, end_time, self.integral_resolution):\n",
    "            #NOTE: evaluate the intensity values at the chosen time points\n",
    "            cur_intensity:torch.Tensor = self.intensity(t, head_predicate_idx, data_sample)\n",
    "            intensity_grid.append(cur_intensity)\n",
    "        #NOTE: approximately calculate the integral\n",
    "        integral = torch.sum(torch.cat(intensity_grid, dim=0) * self.integral_resolution)\n",
    "        return integral\n",
    "\n",
    "    def ELBO(self, prob, action_history, sample_size:int=20)->torch.tensor:\n",
    "        #TODO: compute the ELBO. \n",
    "        #TODO: Maximize the ELBO is equivalent to minimize the KL divergence between the variational posterior and the true posterior\n",
    "        #NOTE: in order to compute the ELBO, we need to 1. be able to sample from the variational posterior; 2. compute the entropy of q\n",
    "        \n",
    "        '''\n",
    "        compute the ELBO (MC estimate)\n",
    "\n",
    "        Parameters:\n",
    "            prob: \n",
    "            action_history: \n",
    "            sample_size: the number of samples we draw from the variational posterior\n",
    "        '''\n",
    "\n",
    "        time_intervals = np.arange(0,self.time_horizon,step=self.time_horizon/self.partition_size)\n",
    "        #TODO: construct two LSTMs to encode the past history\n",
    "        #LSTM_Action = LSTM_Encoding_Action(input_size=,hidden_size=,output_size=)       #NOTE:\n",
    "        #LSTM_History = LSTM_Encoding_History(input_size=,hidden_size=,output_size=)     #NOTE:\n",
    "        mental_history = torch.zeros()\n",
    "        post_samples_collection = []\n",
    "        ELBO = torch.zeros(size=1)\n",
    "\n",
    "        for i in range(len(time_intervals)):\n",
    "            #\n",
    "            #TODO: encode the action history\n",
    "            #h_a = LSTM_Action.forward(action_history)\n",
    "            #TODO: encode the mental history before the i-th time interval (LSTMs)-> categorical distribution -> prob\n",
    "            #prob = LSTM_History.forward(x=mental_history, action_embedding=h_a)\n",
    "            #TODO: post_samples = self.sample_variational_posterior_hard(size = sample_size, prob = prob)\n",
    "            post_samples:list = self.sample_variational_posterior(size = sample_size, prob = prob, type='hard')\n",
    "        #TODO: get the imputed complete dataset\n",
    "        #data = concatenate mental and action\n",
    "        #TODO: ELBO += 1/L * (\\sum log likelihood)\n",
    "        #ELBO += 1/sample_size * (self.log_likelihood(dataset=data,sample_ID_batch=,T_max=self.time_horizon))\n",
    "        #TODO: ELBO = ELBO + self.entropy_variational posterior\n",
    "        ELBO += self.entropy_variational_posterior()\n",
    "        #TODO: return ELBO\n",
    "        return ELBO\n",
    "\n",
    "    def ELBO_Grad_MC_phi(self):\n",
    "        #TODO: compute the gradient of ELBO w.r.t. to the variational parameter 'phi'\n",
    "        pass\n",
    "\n",
    "    def ELBO_Grad_MC_theta(self):\n",
    "        #TODO: compute the gradient of ELBO w.r.t. to the model parameter 'theta'\n",
    "        pass\n",
    "\n",
    "    def sample_variational_posterior(self, size:int, prob:list, type='hard')->list:\n",
    "        #TODO: use gumbel-max trick to explicitly sample from the variational posterior defined by LSTMs, which is a categorical distribution\n",
    "        '''\n",
    "        draw explicit samples from variational posterior\n",
    "\n",
    "        Parameters:\n",
    "            size: number of samples\n",
    "            prob: probabilities of the categorical distribution we want to sample from\n",
    "            type: hard/soft\n",
    "        '''\n",
    "        logits = torch.log(torch.tensor(prob))                                          # compute the \n",
    "        result = []\n",
    "        for i in range(size):\n",
    "            gumbel_samples = self.sample_Gumble(shape=logits.shape)                     # get Gumbel noise\n",
    "            if type != 'hard': tmp = torch.softmax(logits+gumbel_samples,dim=0)         # if 'hard': sample from the categorical distribution\n",
    "            else:                                                                       # sample from the Gumbel-softmax distribution\n",
    "                idx = torch.argmax(logits+gumbel_samples)\n",
    "                tmp = torch.zeros(size=logits.shape)\n",
    "                tmp[idx] = 1\n",
    "            result.append(tmp.numpy())\n",
    "        #NOTE: return a one-hot vector.\n",
    "        return result\n",
    "\n",
    "    def sample_Gumble(self, shape, eps:float=1e-20, tens_type=torch.FloatTensor):\n",
    "        #TODO: sample from Gumbel(0,1). This is needed when we want to explicitly sample from the variational posterior\n",
    "        '''\n",
    "        Sample from Gumbel(0,1) with shape = 'shape'\n",
    "\n",
    "        Parameters:\n",
    "            shape: \n",
    "            eps: small perturbation to avoid log(0)\n",
    "            tens_type: \n",
    "        '''\n",
    "\n",
    "        U = Variable(tens_type(*shape).uniform_(), requires_grad=False)\n",
    "        return -torch.log(-torch.log(U+eps)+eps)\n",
    "\n",
    "    def entropy_variational_posterior(self, temperature:float, prob:list, MC_size:int=100)->float:\n",
    "        #TODO: approximately calculate the entropy of the variational posterior\n",
    "        #TODO: the true variational posterior (categorical) is approximated by a Gumbel-softmax distribution, controlled by 'tau' (temperature)\n",
    "        '''\n",
    "        Parameters:\n",
    "            temperature: temperature parameter\n",
    "            prob: probabilities for each category\n",
    "        '''\n",
    "        #TODO: draw 'MC_size' samples from gumbel softmax distribution\n",
    "        gumbel_softmax_samples = np.array(self.sample_variational_posterior(size=MC_size,prob=prob,type='soft'))\n",
    "        #print(gumbel_softmax_samples)\n",
    "        #TODO: compute the log-densities\n",
    "        log_densities = np.log(self.Gumbel_softmax_density(gumbel_softmax_samples,temperature,prob))\n",
    "        #TODO: this is the Monte-Carlo estimate of the entropy\n",
    "        result = np.mean(-log_densities)\n",
    "        return result\n",
    "\n",
    "    def Gumbel_softmax_density(self, y:np.array, temperature:float, prob:list)->np.array:\n",
    "        #TODO: return the probability density of Gumbel softmax distribution at y\n",
    "        '''\n",
    "        Parameters:\n",
    "            y: input\n",
    "            temperature: temperature parameter\n",
    "            prob: probabilities for each category\n",
    "        '''\n",
    "        k = len(prob)\n",
    "        prob = np.array(prob)\n",
    "        #NOTE: compute the probability density. RHS is the density of gumbel softmax distribution\n",
    "        result = gamma(k) * (temperature)**(k-1) * (np.dot(prob,1/(y.T)**temperature))**(-k) * np.sum(prob/y**(temperature+1))\n",
    "        return result\n",
    "\n",
    "    def train_model(self, data, batch_size:int):\n",
    "        #TODO: train the model from incomplete data by gradient descent\n",
    "        #TODO: 1. draw a minibatch ('batch_size') from the data, compute ELBO\n",
    "        #TODO: 2. compute gradient of ELBO w.r.t. to \\theta (model parameter) and \\psi (variational parameter, i.e. LSTM param)\n",
    "        #TODO: 3. gradient ascent, alternatively optimize \\theta and \\psi\n",
    "        pass\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.13 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "271aaf9a14ca8aef692edd0e58d89184235237d981934b751bc9762a3149e378"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
